We're basically already starting to work on Llama 4 and our goal is to completely close the gap with all the others. Llama 3.1 is probably the biggest news last week - Meta is killing it with their open source play and looks like they are already working on Llama 4 which might drop end of year. Llama 3.1 demonstrates really promising performance across multiple different capabilities like masked coding and instruction following.
One part that I found people are not talking a lot about, but I'm extremely excited about, is that Meta seems to be really investing in the agent-related use cases. They mention that they aim to not just position Llama as a model, but as a system to provide tools that enable developers to build their own custom agents as well as new types of agentic behavior. They have a public report called 'Llama Agentic System' where they showcase whole components of the Llama stack. It includes things like Llama Guard, which is a specialized model trained to moderate content, as well as Prompt Guard to prevent jailbreaks, and CodeSecure to prevent insecure code produced by large language models.
But the most exciting part for me is tool calling, because tool calling is probably so far the main reason I have to use OpenAI, as their models are just way better at tool calling-related agent use cases. If you don't know what tool calling is, it is a concept introduced by OpenAI at the end of last year. This is basically a type of model trained to, given a user task, predict what function needs to be called as well as inputs for this function, so that we can take this JSON output to actually run the function and send information back to the large language model.
It is different from other types of popular agent frameworks like ReAct agents where you use prompts to force large language models to always go through this process of thought, action, observation. They're both great approaches for building autonomous agents, but tool calling has a lot of core benefits - you can actually support calling multiple different functions at the same time instead of doing one by one, and tool calling is generally better because all those model providers will continuously improve their models' tool calling ability instead of optimizing for the ReAct model.
From the initial evaluation results, Llama 3.1 model's tool calling ability seems to perform really well against other models like GPT-4 and Claude 3.5. But the majority of those evaluation benchmarks are kind of zero-shot tool use, which doesn't necessarily represent the actual tool usage performance in real world because it's fairly easy and simple to do zero-shot single tool usage like when a user has a question 'What's the weather in SF?' and you just predict to call one function getWeather with location input.
But the real world agent use cases are a lot more complicated, like multi-turn tool usage where the user task cannot be completed just within calling one tool. It requires some sort of planning and reasoning ability to be able to break down a big task into small steps and then based on the result of each step to predict and plan the best next action. Like if the user query is 'I like hot weather and cheap flights, help me plan a trip that suits me the most at the moment between Tokyo and Bangkok' - you need to call functions for both Tokyo and Bangkok to get weather as well as function getFlightPrice, and based on the result of those four different function calls, generally Tokyo seems to be the best option and then call different functions like bookFlight, bookHotel, and bookCars.
And on top of that, this model will also need to do both those function calling plus conversation, so in the end it probably needs to generate a good report based on all those research findings. So these things are a lot more complicated than just zero-shot tool usage. But the good thing is that in the Llama 3.1 model, it seems like they do train the model specifically for those multi-turn dialogues. So if the query requires multiple tool calls, the model can write a step-by-step plan, call the tools in sequence, and do the reasoning after each tool call.
One thing to note is that for smaller models like Llama 7B, it can't reliably maintain conversation alongside calling yet - it can only reliably be used for zero-shot tool calling, while the 70B and 405B models are more suitable for combining conversation and calling together. And Llama also showcases the actual prompt that has been used to drive those tool calling abilities, and this is really useful because that will help us to understand how it works behind the scenes and you can take this to fine-tune a specific agentic model.
So normal prompts look something like this: at the beginning you give some context about what are the tool names that the model has access to as well as actual tool schema, very similar to how the OpenAI function schema looks like. Then they will give the model a very specific instruction about what kind of result to generate for calling a function. So by default it is something like a tag function with a function name as well as the actual function input details in the middle, and after that it will generate a tag called EOMOM representing end of message.
These two things together are kind of like a system prompt to instruct how the model should behave, and then you can insert a user message into a prompt. Based on all this information, the model generates a function calling output based on the instruction it was given, and we can take this JSON output to actually execute the function. After we get the function result, we can send results back wrapped under a Python array, and based on the information the model will generate an answer.
So this tool calling ability from Llama 3.1 is a really exciting step for us to have an alternative other than OpenAI to have a really strong model for agents. I'm going to show you how you can create agents with the Llama 3.1 model to get a sense of how well it performs.
[Sponsored segment removed]
Let's get back to how we can build a Llama 3.1 AI agent. But what type of use case should we start building with Llama 3.1? Here's a use case that Mark Zuckerberg mentioned in his interview with Bloomberg: 'You know there's almost 200 million creators on our platforms, they all are trying to build their community. People want to interact with them, there aren't enough hours in the day. I want to make it so that every single one of them can easily train like an AI version of themselves. They can make it what they want, so it's almost like a kind of artistic artifact that they're putting out there that allows their community to interact with them but also gives them control over how that interaction happens. And I think that's going to be great and there's going to be millions, eventually hundreds of millions of those.'
So this idea where you have some sort of digitized version of yourself is very interesting because I can definitely feel the pain there. One use case that's adjacent to this digitized creator is company knowledge distribution - for any domain experts who host specific types of knowledge inside a company, how to distribute that information to all the other employees and team members is a really big problem. If you can build an agent to digitize some sort of domain expert in the company, it's almost like having your top performing employees always accessible to everyone 24/7.
This problem was also mentioned by Chez in his podcast: 'If you have a Google Workspace, my entire company runs on Google Workspace, and click a button where all of a sudden now all of that stuff in all of my G Drives all of a sudden is trainable so that the new employee comes in and has an agent that's tuned on every deck, every spreadsheet, every document - that's a huge, huge edge.'
So I really believe that this is an exciting and very big use case, and the beautiful thing is that it's actually very easy and simple to build an agent that lives inside your Slack that can consume all the documentation written by your domain experts using Llama 3.1 that is running fully on your local machine using Ollama. We can build this Llama agent that is not only doing knowledge retrieval but also continuously improving itself when observing new knowledge and eventually automating some simple repetitive tasks just like another team member or employee.
So I'm going to show you step by step how you can build such an agent in the next 5 minutes:
Step 1: Setup Llama 3.1 locally
First, we want to download the Llama 3.1 model on your local machine and we're going to use Ollama, which is a package that allows you to run those open source large language models on your local machine. If you have Ollama already installed, you can open Terminal and type 'ollama pull llama3.1'. This by default will download the 7B model which should be small enough to be able to run on your MacBook Pro. After it is downloaded, you can do 'ollama run llama3.1'. You can see this model is working on my MacBook Pro with pretty decent speed.
Step 2: Setup RAG pipeline
The next step is that we want to build a Llama 3.1 agent that exists in your Slack workspace and can be tagged to answer questions and automate tasks, and most importantly distribute domain knowledge to different employees and team members on demand. To do that, we firstly need to decide how we want to train large language models with our private data. Commonly there are two ways: you can either fine-tune the Llama 3.1 model or you can build a RAG pipeline.
There are pros and cons for each method. Well, fine-tuning's benefit is it's going to be faster, but it's also a more involved process - you need to prepare a lot of data to make it work and also the model's knowledge will be frozen at the time when it is trained, so if you have new data that you want to bring into the model, you kind of need to fine-tune again. Versus RAG is a method that's way easier to start and also supports scenarios where you have a dynamic knowledge base like Notion or Confluence very well, and this is the path I'm going to take for the Llama agent.
Then we need to decide how we want to build this RAG pipeline. Traditionally most of the time people kind of start building the RAG pipeline by using open source frameworks like LlamaIndex, LangChain and open source vector storage like Chroma because it is easy to start. However, the tricky part of RAG is that there are thousands of different techniques you can try to make RAG better, and there's actually a bunch of emerging fully managed RAG pipeline platforms that are doing all those optimizations for you and also have benefits where they can schedule to automatically reindex the database so whenever you add new stuff to your Notion or Confluence page, the index can always be kept up to date.
There are platforms like LlamaCloud, Carbon AI, and for my purpose I actually don't want to manage this RAG pipeline - I just want to outsource to someone else. That's why I'm going to use one of the platforms, LlamaCloud, which is a fully managed RAG pipeline platform built by the LlamaIndex team. Because the whole platform is built on top of LlamaIndex, it is a lot more transparent to understand what is going on behind the scenes and fairly easy to migrate over if you're already using LlamaIndex. They also provide a playground for you to play with different combinations between chunk size, reranking, metadata filtering and other techniques that have been optimized.
So the first step is I want to turn my Notion knowledge base into a LlamaCloud RAG pipeline. When I go to LlamaCloud, I can create an index, and index is like a data source. I can give it a name called 'notion database' and then I can select the data source - it can be PDF files which will connect to their LlamaParse which is really good, or they can support Amazon S3 bucket, Azure, OneDrive, SharePoint, Slack, Notion, and Git. The one I want to use is Notion.
It will ask you for a few pieces of information with the integration token. To get the integration token you will go to notion.so/my-integrations, create an app in Notion, select the workspace that you want to connect data to and click save. Then you will go to integration settings, copy over this internal integration secret and paste it here into LlamaCloud. But one thing you need to know is that in Notion after you set up the integration, you actually need to go to the page and workspace where you want to give access to knowledge, click on the three dots button and choose 'Connect to' and select the app that you just created. Only after this can the application have access to specific content and pages that connect to the app can be retrieved from LlamaCloud.
You can also specify which specific database ID as well as page ID to scope down the amount of knowledge that it should retrieve. In the URL this is the database ID - same thing for the page ID, you can just go to a page that you want to share and you will see in the page URL the list of numbers - that unique ID is the page ID. After that you can select where you want to save the data - it can be fully managed or use Python or other data storage as well as embedding model. It will start fetching data and indexing the whole Notion database and then you can just retrieve information directly from this API endpoint very easily.
Step 3: Slack Integration
Next is we want to connect this RAG pipeline into Slack and to do that we need to quickly create a custom Slack bot. You can go to Slack, click on Settings and Manage Apps, then click on the Build button on top right corner and click on Create a New App. We will just do 'From scratch' and give it a name 'Jason Bot', choose the workspace where you want to develop this app and go to OAuth & Permissions. I add a few different scopes: from reading chat history in channels and groups, sending messages and group messages, as well as adding and reading reactions to messages, viewing people in the workspace, and also sending out messages. So this should be enough scope for us to build a lot of interesting interactions.
Then we will try to install the app to your workspace - click Allow, then you will get an OAuth token that you can copy and use later. But for now if you go to your Slack and try to @mention your bot name, you can see this bot already exists but it didn't do anything yet.
Step 4: Connect Llama 3.1 agent to Slack & Add self-learning ability
Next step is we want to connect this bot to Llama 3.1 model on your local machine via Ollama as well as build those advanced functionalities like knowledge retrieval and learning ability. So let's open Visual Studio Code. First is set up a .env file - this is where we store all the credentials that we need from Slack, Notion, LlamaCloud as well as Fireworks which is a large language model inference service that can allow you to access like Llama 70B model.
The Slack bot token is the OAuth token that you will get in OAuth & Permissions while the Slack signing secret is the Signing Secret under the Basic Information here. And we're going to get the Slack bot user ID by calling an API endpoint. So I will import a few different libraries as well as credentials we just added and create a function called getBotUserId(). This will test whether your Slack authentication is successful.
So I will open Terminal and then do 'python app.py' then get this bot user ID which you can paste over to the .env file and that also means the connection is successful. So we can basically comment out this part then add a few new functions. So I have one function called postMessage that allows the bot to post messages via Slack as well as add a reaction to the message, remove reaction to the message so I can create some sort of interesting interactions. We also need helper functions to get username based on user ID as well as fetch the whole thread conversation history.
And in the end I will create two event listeners - one is when the bot is @mentioned as well as when bot receives a message. In the end I will set up a webhook endpoint for Slack to send messages to us. So this is a very basic simple function that every time when you receive a message it will just reply back "yo". So now you can see that it is up and running.
We will also need to add another terminal and we're going to use ngrok. Ngrok is a service that can put our local running endpoint to public internet so I will do 'ngrok http' which is the port that we set up here. So now you can see this endpoint has been set up publicly.
So next we want Slack to send us all this information whenever there's a new message. So I go to Event Subscriptions, turn on Enable Events and then you can paste in the URL. One thing to note is that for this endpoint you will need to change it a little bit to add this challenge response back for the validation, otherwise this won't be verified. And then we add a few different events to listen to: from app_mention, message.channels, message.groups, im and message.mpim so that the agent will receive messages whenever it is @mentioned or some new message from existing chat that it's already involved in.
Okay great, so now I can give it a test. I can go to my Slack and then add this bot, give a message - you can see it responds back with this message so it is working well. The next thing is we want to start building an agent that can retrieve knowledge and respond properly. To do that I will create a new file called functions.py and this is where we will create agents.
So I'm going to import a few different libraries in here - I'm using the LlamaIndex and Ollama. Ollama is a package that allows you to run this open source model on your local machine but I also import Fireworks which is the large language model inference system that allows you to run Llama 3